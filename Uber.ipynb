{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_space_separated_csv(file_path, column_names):\n",
    "    return pd.read_csv(file_path, sep='\\t', names=column_names)\n",
    "\n",
    "def process_poi_data(poi_data_str):\n",
    "    poi_data_dict = {}\n",
    "    for pair in poi_data_str.split('\\t'):\n",
    "        key, value = pair.split(':')\n",
    "        poi_data_dict[key] = int(value)\n",
    "    return poi_data_dict\n",
    "\n",
    "def read_poi_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.read().split('\\n')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            region_hash, poi_data_str = line.split('\\t', 1)\n",
    "            poi_data_dict = process_poi_data(poi_data_str)\n",
    "            data.append({'region_hash': region_hash, 'poi_data': poi_data_dict})\n",
    "\n",
    "    column_names = ['region_hash', 'poi_data']\n",
    "    poi_data_df = pd.DataFrame(data, columns=column_names)\n",
    "    return poi_data_df\n",
    "\n",
    "\n",
    "file_path = 'training_data/poi_data/poi_data'\n",
    "poi_data = read_poi_data(file_path)\n",
    "\n",
    "# Read the cluster_map data\n",
    "cluster_map_columns = ['region_hash', 'region_id']\n",
    "cluster_map_file_path = 'training_data/cluster_map/cluster_map'\n",
    "cluster_map = read_space_separated_csv(cluster_map_file_path, cluster_map_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the order_data files\n",
    "order_data_columns = ['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'Price', 'Time']\n",
    "order_data_folder_path = 'training_data/order_data/'\n",
    "order_data_files = [f for f in os.listdir(order_data_folder_path) if os.path.isfile(os.path.join(order_data_folder_path, f)) and not f.startswith('._')]\n",
    "order_data_list = [read_space_separated_csv(os.path.join(order_data_folder_path, file), order_data_columns) for file in order_data_files]\n",
    "order_data = pd.concat(order_data_list)\n",
    "\n",
    "\n",
    "# Read the weather_data files\n",
    "weather_data_columns = ['Time', 'Weather', 'temperature', 'PM2.5']\n",
    "weather_data_folder_path = 'training_data/weather_data/'\n",
    "weather_data_files = [f for f in os.listdir(weather_data_folder_path) if os.path.isfile(os.path.join(weather_data_folder_path, f)) and not f.startswith('._')]\n",
    "weather_data_list = [read_space_separated_csv(os.path.join(weather_data_folder_path, file), weather_data_columns) for file in weather_data_files]\n",
    "weather_data = pd.concat(weather_data_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['PM2.5', 'Weather', 'temperature'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m merged_data[\u001b[39m'\u001b[39m\u001b[39mday\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m merged_data[\u001b[39m'\u001b[39m\u001b[39mTime\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mdayofweek\n\u001b[0;32m     19\u001b[0m \u001b[39m# Aggregate the data to calculate demand, supply, and gap\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m demand_supply \u001b[39m=\u001b[39m merged_data\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mregion_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtime_slot\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49magg({\n\u001b[0;32m     21\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39morder_id\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mcount\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     22\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mdriver_id\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mcount\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     23\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mWeather\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     24\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     25\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mPM2.5\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mday\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     27\u001b[0m })\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m     29\u001b[0m demand_supply\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39morder_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdemand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdriver_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msupply\u001b[39m\u001b[39m'\u001b[39m}, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m demand_supply[\u001b[39m'\u001b[39m\u001b[39mgap\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m demand_supply[\u001b[39m'\u001b[39m\u001b[39mdemand\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m demand_supply[\u001b[39m'\u001b[39m\u001b[39msupply\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:894\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[0;32m    893\u001b[0m op \u001b[39m=\u001b[39m GroupByApply(\u001b[39mself\u001b[39m, func, args, kwargs)\n\u001b[1;32m--> 894\u001b[0m result \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49magg()\n\u001b[0;32m    895\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dict_like(func) \u001b[39mand\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\pandas\\core\\apply.py:169\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m is_dict_like(arg):\n\u001b[1;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magg_dict_like()\n\u001b[0;32m    170\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(arg):\n\u001b[0;32m    171\u001b[0m     \u001b[39m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\pandas\\core\\apply.py:478\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     selected_obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_selected_obj\n\u001b[0;32m    476\u001b[0m     selection \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_selection\n\u001b[1;32m--> 478\u001b[0m arg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize_dictlike_arg(\u001b[39m\"\u001b[39;49m\u001b[39magg\u001b[39;49m\u001b[39m\"\u001b[39;49m, selected_obj, arg)\n\u001b[0;32m    480\u001b[0m \u001b[39mif\u001b[39;00m selected_obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    481\u001b[0m     \u001b[39m# key only used for output\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     colg \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_gotitem(selection, ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\pandas\\core\\apply.py:601\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[1;34m(self, how, obj, func)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cols) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    600\u001b[0m         cols_sorted \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(safe_sort(\u001b[39mlist\u001b[39m(cols)))\n\u001b[1;32m--> 601\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn(s) \u001b[39m\u001b[39m{\u001b[39;00mcols_sorted\u001b[39m}\u001b[39;00m\u001b[39m do not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    603\u001b[0m aggregator_types \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mdict\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[39m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[39m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[39m# be list-likes\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[39m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column(s) ['PM2.5', 'Weather', 'temperature'] do not exist\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Your code to read data goes here...\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "\n",
    "# Convert the 'Time' column to datetime format\n",
    "order_data['Time'] = pd.to_datetime(order_data['Time'])\n",
    "weather_data['Time'] = pd.to_datetime(weather_data['Time'])\n",
    "\n",
    "# Extract hour, minute, and day of the week from the 'Time' column\n",
    "order_data['hour'] = order_data['Time'].dt.hour\n",
    "order_data['minute'] = order_data['Time'].dt.minute\n",
    "order_data['day_of_week'] = order_data['Time'].dt.dayofweek\n",
    "\n",
    "# Merge the cluster_map into the order_data\n",
    "order_data = order_data.merge(cluster_map, left_on='start_region_hash', right_on='region_hash', how='left')\n",
    "order_data = order_data.rename(columns={'region_id': 'start_region_id'}).drop('region_hash', axis=1)\n",
    "\n",
    "order_data = order_data.merge(cluster_map, left_on='dest_region_hash', right_on='region_hash', how='left')\n",
    "order_data = order_data.rename(columns={'region_id': 'dest_region_id'}).drop('region_hash', axis=1)\n",
    "\n",
    "# Merge the weather_data into the order_data\n",
    "order_data = order_data.merge(weather_data, on='Time', how='left')\n",
    "\n",
    "# Calculate the demand and supply columns\n",
    "order_data['demand'] = 1\n",
    "order_data['supply'] = order_data['driver_id'].notnull().astype(int)\n",
    "\n",
    "# Aggregate data by time slot and region\n",
    "grouped_data = order_data.groupby(['Time', 'start_region_id']).agg({'demand': 'sum', 'supply': 'sum'}).reset_index()\n",
    "grouped_data['gap'] = grouped_data['demand'] - grouped_data['supply']\n",
    "\n",
    "# Prepare the features and target variable\n",
    "X = grouped_data.drop(['Time', 'gap', 'demand', 'supply'], axis=1)\n",
    "y = grouped_data['gap']\n",
    "# Encode the region_id column\n",
    "le = LabelEncoder()\n",
    "X['start_region_id'] = le.fit_transform(X['start_region_id'])\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the demand-supply gap for the testing dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean absolute error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- day_of_week\n- hour\n- minute\nFeature names seen at fit time, yet now missing:\n- demand\n- supply\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m         test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([test_data])\n\u001b[0;32m     31\u001b[0m         test_df[\u001b[39m'\u001b[39m\u001b[39mstart_region_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mtransform(test_df[\u001b[39m'\u001b[39m\u001b[39mstart_region_id\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m         prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(test_df\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mTime\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m     34\u001b[0m         results\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mRegion_id\u001b[39m\u001b[39m'\u001b[39m: region_id, \u001b[39m'\u001b[39m\u001b[39mTime_slot\u001b[39m\u001b[39m'\u001b[39m: time_slot, \u001b[39m'\u001b[39m\u001b[39mPrediction_value\u001b[39m\u001b[39m'\u001b[39m: prediction})\n\u001b[0;32m     36\u001b[0m \u001b[39m# Save the predictions in the required format\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\sklearn\\linear_model\\_base.py:354\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    341\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\sklearn\\linear_model\\_base.py:337\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    335\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 337\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\sklearn\\base.py:529\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[0;32m    465\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    466\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[0;32m    471\u001b[0m ):\n\u001b[0;32m    472\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    533\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\syeda\\anaconda3\\envs\\ana\\lib\\site-packages\\sklearn\\base.py:462\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    458\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    459\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m     )\n\u001b[1;32m--> 462\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- day_of_week\n- hour\n- minute\nFeature names seen at fit time, yet now missing:\n- demand\n- supply\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Prepare the features and target variable\n",
    "# X = grouped_data.drop(['Time', 'gap'], axis=1)\n",
    "# y = grouped_data['gap']\n",
    "\n",
    "# # Encode the region_id column\n",
    "# le = LabelEncoder()\n",
    "# X['start_region_id'] = le.fit_transform(X['start_region_id'])\n",
    "\n",
    "# # Train the linear regression model on the entire dataset\n",
    "# model = LinearRegression()\n",
    "# model.fit(X, y)\n",
    "\n",
    "# # Generate predictions for each region and time slot in the test dataset\n",
    "# test_date = pd.to_datetime('2016-01-23')\n",
    "# test_hour_slots = [test_date + timedelta(minutes=10 * i) for i in range(144)]\n",
    "\n",
    "# results = []\n",
    "# for region_id in cluster_map['region_id'].unique():\n",
    "#     for time_slot in test_hour_slots:\n",
    "#         test_data = {\n",
    "#             'Time': time_slot,\n",
    "#             'start_region_id': region_id,\n",
    "#             'hour': time_slot.hour,\n",
    "#             'minute': time_slot.minute,\n",
    "#             'day_of_week': time_slot.dayofweek\n",
    "#         }\n",
    "        \n",
    "#         test_df = pd.DataFrame([test_data])\n",
    "#         test_df['start_region_id'] = le.transform(test_df['start_region_id'])\n",
    "        \n",
    "#         prediction = model.predict(test_df.drop('Time', axis=1))[0]\n",
    "#         results.append({'Region_id': region_id, 'Time_slot': time_slot, 'Prediction_value': prediction})\n",
    "\n",
    "# # Save the predictions in the required format\n",
    "# predictions_df = pd.DataFrame(results)\n",
    "# predictions_df['Time_slot'] = predictions_df['Time_slot'].apply(lambda x: x.strftime('%Y-%m-%d-%H-%M'))\n",
    "# predictions_df.to_csv('predictions.csv', index=False, columns=['Region_id', 'Time_slot', 'Prediction_value'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
